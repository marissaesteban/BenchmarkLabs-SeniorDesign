{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "# import libraries needed\n",
    "from seebuoy import NDBC\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew, probplot\n",
    "from scipy.special import boxcox1p\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from datetime import datetime, timedelta\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error (RMSE).\n",
    "    \n",
    "    Parameters:\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "        \n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        Estimated target values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The RMSE value.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buoySetUp(buoyNum):  \n",
    "    ndbc = NDBC(timeframe=\"historical\")\n",
    "    df_avail = ndbc.available_data(station_id=buoyNum)\n",
    "    df_data = ndbc.get_data(buoyNum)\n",
    "        \n",
    "    df_data.dropna(axis=1, how='all', inplace=True)\n",
    "    df_data = df_data.reset_index()\n",
    "\n",
    "    if 'wave_height' not in df_data.columns or 'average_period' not in df_data.columns:\n",
    "        print(\"Not enough data\")\n",
    "        return False\n",
    "\n",
    "    # lets limit the df to 2 columns: date and wave height\n",
    "    buoy_df = df_data[[\"date\",\"wave_height\", \"average_period\"]]\n",
    "\n",
    "    # Set 'date' column as the index\n",
    "    buoy_df = buoy_df.set_index(\"date\")\n",
    "\n",
    "    buoy_df['wave_height_interpolated'] = buoy_df['wave_height'].interpolate(method='time') # interpolate missing values based on time\n",
    "    buoy_df['average_period_interpolated'] = buoy_df['average_period'].interpolate(method='time') # interpolate missing values based on time\n",
    "\n",
    "    return buoy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mbuoySetUp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdjkszfhuidst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mbuoySetUp\u001b[0;34m(buoyNum)\u001b[0m\n\u001b[1;32m      2\u001b[0m ndbc \u001b[38;5;241m=\u001b[39m NDBC(timeframe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_avail \u001b[38;5;241m=\u001b[39m ndbc\u001b[38;5;241m.\u001b[39mavailable_data(station_id\u001b[38;5;241m=\u001b[39mbuoyNum)\n\u001b[0;32m----> 4\u001b[0m df_data \u001b[38;5;241m=\u001b[39m \u001b[43mndbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuoyNum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_data\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m df_data \u001b[38;5;241m=\u001b[39m df_data\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/Desktop/2024S/COMP492/Benchmark-Code/.venv/lib/python3.11/site-packages/seebuoy/ndbc/ndbc.py:159\u001b[0m, in \u001b[0;36mNDBC.get_data\u001b[0;34m(self, station_id, dataset, rename_cols, drop_duplicates)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtimeframe is not real_time, current_year, or historical.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m     df_store\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m--> 159\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(df_store)\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m drop_duplicates:\n\u001b[1;32m    162\u001b[0m     df \u001b[39m=\u001b[39m df[\u001b[39m~\u001b[39mdf\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mduplicated(keep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/Desktop/2024S/COMP492/Benchmark-Code/.venv/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/2024S/COMP492/Benchmark-Code/.venv/lib/python3.11/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    369\u001b[0m         objs,\n\u001b[1;32m    370\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    371\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    372\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    373\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    374\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    375\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    376\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    377\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    378\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/Desktop/2024S/COMP492/Benchmark-Code/.venv/lib/python3.11/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "test = buoySetUp(\"djkszfhuidst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(f, c, target, buoyNum):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    f : floor value, how many days back to train the model on\n",
    "\n",
    "    c : ceiling value, how many days we want to predict for (want 10-15 days)\n",
    "\n",
    "    target : either \"wave_height\" or \"average_period\". Variable we want to traiun and predict on.\n",
    "    \"\"\"\n",
    "    buoy_df = buoySetUp(buoyNum)\n",
    "\n",
    "    if type(buoy_df) == bool:\n",
    "        return\n",
    "\n",
    "    buoy_df = buoy_df.reset_index()\n",
    "\n",
    "    if target != \"wave_height\" and target != \"average_period\":\n",
    "        print(\"Not a valid target variable\")\n",
    "        return\n",
    "\n",
    "    #Sets up date objects and floor and ceiling\n",
    "    today_date = datetime.today().date()\n",
    "    floor = f + c\n",
    "    ceiling = c\n",
    "\n",
    "    # Calculate the floordate and the ceiling date\n",
    "    floorDate = today_date - timedelta(days=floor)\n",
    "    ceilingDate = today_date - timedelta(days=ceiling)\n",
    "\n",
    "    # Splits up the df into recent and past\n",
    "    # Recent holds the 15 most recent days of data\n",
    "    # Past holds all data from the floor to the ceiling\n",
    "    recent_df = buoy_df[buoy_df['date'] > pd.Timestamp(ceilingDate)]\n",
    "    past_df = buoy_df[(buoy_df['date'] > pd.Timestamp(floorDate)) & (buoy_df['date'] < pd.Timestamp(ceilingDate))]\n",
    "\n",
    "    #Sets up the modeling df with the date and target variable column as well as the cap for logistic growth prophet algo\n",
    "    modeling_df = past_df[[\"date\",f'{target}_interpolated']]\n",
    "    modeling_df = modeling_df.rename(columns={\"date\": \"ds\", f'{target}_interpolated': \"y\"})\n",
    "    cap = modeling_df['y'].max() + 1\n",
    "\n",
    "    # Initialize Prophet model with the cap\n",
    "    model = Prophet(growth=\"logistic\")\n",
    "    modeling_df['cap'] = cap\n",
    "    model.fit(modeling_df)\n",
    "\n",
    "    #Makes prediction\n",
    "    future = model.make_future_dataframe(periods=ceiling)\n",
    "    future['cap'] = cap\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    forecast['date'] = forecast['ds']\n",
    "    merged_df = pd.merge(forecast.tail(15), recent_df, on='date', how='left')\n",
    "\n",
    "    print(f\"RMSE for {buoyNum} for {ceiling} days using {floor - 15} days worth of training data for {target}\")\n",
    "    print(rmse(merged_df[f'{target}_interpolated'], merged_df[\"yhat\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data\n"
     ]
    }
   ],
   "source": [
    "doit(365, 15, \"wave_height\", \"41064\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:34:26 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 46239 for 15 days using 365 days worth of training data for average_period\n",
      "1.5782466136886855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:34:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 46258 for 15 days using 365 days worth of training data for average_period\n",
      "1.7075612441443435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:35:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 46232 for 15 days using 365 days worth of training data for average_period\n",
      "1.43403613958504\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:36:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 42001 for 15 days using 365 days worth of training data for average_period\n",
      "1.217700567548797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:38:13 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 41002 for 15 days using 365 days worth of training data for average_period\n",
      "1.0701111993389985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:38:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 44097 for 15 days using 365 days worth of training data for average_period\n",
      "1.1053278190668125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:39:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:40:13 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 44009 for 15 days using 365 days worth of training data for average_period\n",
      "0.8160995611196501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buoy_list = [\"46239\", \"46258\", \"46232\", \"42001\", \"41002\", \"44097\", \"44009\"]\n",
    "\n",
    "\n",
    "for buoyNum in buoy_list:\n",
    "    doit(365, 15, \"average_period\", buoyNum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "418da822878bf5e9e939dfb4ad40fe672876b0467fe6e10ad67d81e9a42cb991"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
